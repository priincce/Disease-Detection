{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GpY-ArzB1rH-"
      },
      "source": [
        "# **Disease Detection using Symptoms and Treatment recommendation**\n",
        "\n",
        "This notebook contains the application of Neural Net and GAN on the disease dataset generated through scrapping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "colab_type": "code",
        "id": "etxuEgYCG7bC",
        "outputId": "b1aa83f5-0ac1-4c9b-b532-575fc01660c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\princ\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\princ\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# importing nltk to download resources for stopwords and wordnet\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CrLG2ksh5w4Z"
      },
      "outputs": [],
      "source": [
        "# importing all libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "import math\n",
        "import operator\n",
        "import pickle\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from statistics import mean\n",
        "from nltk.corpus import wordnet \n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from itertools import combinations\n",
        "from time import time\n",
        "from collections import Counter\n",
        "import operator\n",
        "import warnings\n",
        "from Treatment import diseaseDetail\n",
        "# ignore warnings generated due to usage of old version of tensorflow\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t1sbUx8C22zG"
      },
      "source": [
        "**Disease Symptom dataset** was created in a separate python program.\n",
        "\n",
        "**Dataset scrapping** was done using **NHP website** and **wikipedia data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "txTu6XeVgGAK"
      },
      "outputs": [],
      "source": [
        "# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia\n",
        "# Scrapping and creation of dataset csv is done in a separate program\n",
        "df=pd.read_csv(\"./Dataset/dis_sym_dataset_norm.csv\")\n",
        "documentname_list=list(df['label_dis'])\n",
        "df=df.iloc[:,1:]\n",
        "columns_name=list(df.columns)\n",
        "documentname_list=list(documentname_list)\n",
        "\n",
        "N=len(df)\n",
        "M=len(columns_name)\n",
        "\n",
        "# All symptoms IDF\n",
        "idf={}\n",
        "for col in columns_name:\n",
        "  temp=np.count_nonzero(df[col])\n",
        "  idf[col]=np.log(N/temp)\n",
        "\n",
        "# All disease,symptom TF\n",
        "tf={}\n",
        "for i in range(N):\n",
        "  for col in columns_name:\n",
        "    key=(documentname_list[i],col)\n",
        "    tf[key]=df.loc[i,col]\n",
        "\n",
        "# All disease,symptom TF.IDF\n",
        "tf_idf={}\n",
        "for i in range(N):\n",
        "  for col in columns_name:\n",
        "    key=(documentname_list[i],col)\n",
        "    tf_idf[key]=float(idf[col])*float(tf[key])\n",
        "\n",
        "# vector of TF.IDF\n",
        "D = np.zeros((N, M),dtype='float32')\n",
        "for i in tf_idf:\n",
        "    sym = columns_name.index(i[1])\n",
        "    dis=documentname_list.index(i[0])\n",
        "    D[dis][sym] = tf_idf[i]\n",
        "\n",
        "# function for cosine dot product\n",
        "def cosine_dot(a, b):\n",
        "    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        temp = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "        return temp\n",
        "\n",
        "# convert data to lower case\n",
        "def convert_tolowercase(data):\n",
        "    return data.lower()\n",
        "\n",
        "# tokenizing using regextokenizer\n",
        "def regextokenizer_func(data):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    data = tokenizer.tokenize(data)\n",
        "    return data\n",
        "\n",
        "# function to generate query vector for tf_idf\n",
        "def gen_vector(tokens):\n",
        "    Q = np.zeros(M)\n",
        "    counter = Counter(tokens)\n",
        "    query_weights = {}\n",
        "    for token in np.unique(tokens):\n",
        "        tf = counter[token]\n",
        "        try:\n",
        "          idf_temp=idf[token]\n",
        "        except:\n",
        "          pass\n",
        "        try:\n",
        "            ind = columns_name.index(token)\n",
        "            Q[ind] = tf*idf_temp\n",
        "        except:\n",
        "            pass\n",
        "    return Q\n",
        "\n",
        "# function to calculate tf_idf_score\n",
        "def tf_idf_score(k, query):\n",
        "    query_weights = {}\n",
        "    for key in tf_idf:\n",
        "        if key[1] in query:\n",
        "            try:\n",
        "                query_weights[key[0]] += tf_idf[key]\n",
        "            except:\n",
        "                query_weights[key[0]] = tf_idf[key]\n",
        "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
        "  \n",
        "    l = []\n",
        "    for i in query_weights[:k]:\n",
        "        l.append(i)\n",
        "    return l\n",
        "\n",
        "# function to calculte Cosine Similarity \n",
        "def cosine_similarity(k, query):\n",
        "    d_cosines = []\n",
        "    query_vector = gen_vector(query)\n",
        "    for d in D:\n",
        "        d_cosines.append(cosine_dot(query_vector, d))\n",
        "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
        "  \n",
        "    final_display_disease={}\n",
        "    for lt in set(out):\n",
        "      final_display_disease[lt] = float(d_cosines[lt])\n",
        "    return final_display_disease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Pyd1WbBq5Ngh"
      },
      "outputs": [],
      "source": [
        "# returns the list of synonyms of the input word from thesaurus.com (https://www.thesaurus.com/) and wordnet (https://www.nltk.org/howto/wordnet.html)\n",
        "def synonyms(term):\n",
        "    synonyms = []\n",
        "    response = requests.get('https://www.thesaurus.com/browse/{}'.format(term))\n",
        "    soup = BeautifulSoup(response.content,  \"html.parser\")\n",
        "    try:\n",
        "        container=soup.find('section', {'class': 'MainContentContainer'}) \n",
        "        row=container.find('div',{'class':'css-191l5o0-ClassicContentCard'})\n",
        "        row = row.find_all('li')\n",
        "        for x in row:\n",
        "            synonyms.append(x.get_text())\n",
        "    except:\n",
        "        None\n",
        "    for syn in wordnet.synsets(term):\n",
        "        synonyms+=syn.lemma_names()\n",
        "    return set(synonyms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cXPUlgHi63Zu"
      },
      "outputs": [],
      "source": [
        "# instantiate objects of libraries\n",
        "splitter = RegexpTokenizer(r'\\w+')\n",
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DGtPYRLhey0y"
      },
      "source": [
        "**Disease Symptom dataset** was created in a separate python program.\n",
        "\n",
        "**Dataset scrapping** was done using **NHP website** and **wikipedia data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rZTXyRhNgN_O"
      },
      "source": [
        "Disease Combination dataset contains the combinations for each of the disease present in dataset as practically it is often observed that it is not necessary for a person to have a disease when all the symptoms are faced by the patient or the user.\n",
        "\n",
        "*To tackle this problem, combinations are made with the symptoms for each disease.*\n",
        "\n",
        " **This increases the size of the data exponentially and helps the model to predict the disease with much better accuracy.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h1LSI08aiDTn"
      },
      "source": [
        "*df_comb -> Dataframe consisting of dataset generated by combining symptoms for each disease.*\n",
        "\n",
        "*df_norm -> Dataframe consisting of dataset which contains a single row for each diseases with all the symptoms for that corresponding disease.*\n",
        "\n",
        "**Dataset contains 261 diseases and their symptoms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hpK73qQx5NmJ"
      },
      "outputs": [],
      "source": [
        "# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia\n",
        "# Scrapping and creation of dataset csv is done in a separate program\n",
        "df_comb = pd.read_csv(\"./Dataset/dis_sym_dataset_comb.csv\") # Disease combination\n",
        "df_norm = pd.read_csv(\"./Dataset/dis_sym_dataset_norm.csv\") # Individual Disease\n",
        "Y = df_norm.iloc[:, 0:1]\n",
        "X = df_norm.iloc[:, 1:]\n",
        "# List of symptoms\n",
        "dataset_symptoms = list(X.columns)\n",
        "diseases = list(set(Y['label_dis']))\n",
        "diseases.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "colab_type": "code",
        "id": "yd8K2QeH5NsL",
        "outputId": "62571279-7471-4698-f233-6e0a97c34979"
      },
      "outputs": [],
      "source": [
        "# Taking symptoms from user as input\n",
        "# Preprocessing the input symtoms \n",
        "user_symptoms = str(input(\"\\nPlease enter symptoms separated by comma(,):\\n\")).lower().split(',')\n",
        "processed_user_symptoms=[]\n",
        "for sym in user_symptoms:\n",
        "    sym=sym.strip()\n",
        "    sym=sym.replace('-',' ')\n",
        "    sym=sym.replace(\"'\",'')\n",
        "    sym = ' '.join([lemmatizer.lemmatize(word) for word in splitter.tokenize(sym)])\n",
        "    processed_user_symptoms.append(sym)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "colab_type": "code",
        "id": "THRf6-Wa5Nxu",
        "outputId": "122d0bcb-569f-4610-f713-76382d03fb3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After query expansion done by using the symptoms entered\n",
            "['ii 2 deuce two II']\n"
          ]
        }
      ],
      "source": [
        "# Taking each user symptom and finding all its synonyms and appending it to the pre-processed symptom string\n",
        "user_symptoms = []\n",
        "for user_sym in processed_user_symptoms:\n",
        "    user_sym = user_sym.split()\n",
        "    str_sym = set()\n",
        "    for comb in range(1, len(user_sym)+1):\n",
        "        for subset in combinations(user_sym, comb):\n",
        "            subset=' '.join(subset)\n",
        "            subset = synonyms(subset) \n",
        "            str_sym.update(subset)\n",
        "    str_sym.add(' '.join(user_sym))\n",
        "    user_symptoms.append(' '.join(str_sym).replace('_',' '))\n",
        "# query expansion performed by joining synonyms found for each symptoms initially entered\n",
        "print(\"After query expansion done by using the symptoms entered\")\n",
        "print(user_symptoms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7sPyVlJIjdv2"
      },
      "source": [
        "The below procedure is performed in order to show the symptom synonmys found for the symptoms entered by the user.\n",
        "\n",
        "The symptom synonyms and user symptoms are matched with the symptoms present in dataset. Only the symptoms which matches the symptoms present in dataset are shown back to the user. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tFUYSnLU5Nu-"
      },
      "outputs": [],
      "source": [
        "# Loop over all the symptoms in dataset and check its similarity score to the synonym string of the user-input \n",
        "# symptoms. If similarity>0.5, add the symptom to the final list\n",
        "found_symptoms = set()\n",
        "for idx, data_sym in enumerate(dataset_symptoms):\n",
        "    data_sym_split=data_sym.split()\n",
        "    for user_sym in user_symptoms:\n",
        "        count=0\n",
        "        for symp in data_sym_split:\n",
        "            if symp in user_sym.split():\n",
        "                count+=1\n",
        "        if count/len(data_sym_split)>0.5:\n",
        "            found_symptoms.add(data_sym)\n",
        "found_symptoms = list(found_symptoms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "colab_type": "code",
        "id": "jBV1qPFv5NpY",
        "outputId": "bc96a52e-15fc-496a-c333-5d195a4fe430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top matching symptoms from your search!\n"
          ]
        }
      ],
      "source": [
        "# Print all found symptoms\n",
        "print(\"Top matching symptoms from your search!\")\n",
        "for idx, symp in enumerate(found_symptoms):\n",
        "    print(idx,\":\",symp)\n",
        "\n",
        "# Show the related symptoms found in the dataset and ask user to select among them\n",
        "select_list = input(\"\\nPlease select the relevant symptoms. Enter indices (separated-space):\\n\").split()\n",
        "\n",
        "# Find other relevant symptoms from the dataset based on user symptoms based on the highest co-occurance with the\n",
        "# ones that is input by the user\n",
        "dis_list = set()\n",
        "final_symp = [] \n",
        "counter_list = []\n",
        "for idx in select_list:\n",
        "    symp=found_symptoms[int(idx)]\n",
        "    final_symp.append(symp)\n",
        "    dis_list.update(set(df_norm[df_norm[symp]==1]['label_dis']))\n",
        "   \n",
        "for dis in dis_list:\n",
        "    row = df_norm.loc[df_norm['label_dis'] == dis].values.tolist()\n",
        "    row[0].pop(0)\n",
        "    for idx,val in enumerate(row[0]):\n",
        "        if val!=0 and dataset_symptoms[idx] not in final_symp:\n",
        "            counter_list.append(dataset_symptoms[idx])\n",
        "\n",
        "# Symptoms that co-occur with the ones selected by user              \n",
        "dict_symp = dict(Counter(counter_list))\n",
        "dict_symp_tup = sorted(dict_symp.items(), key=operator.itemgetter(1),reverse=True)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "colab_type": "code",
        "id": "SgvzIn7Q5NjV",
        "outputId": "d0da9e18-ff49-4cd1-f486-983f06140917"
      },
      "outputs": [],
      "source": [
        "# Iteratively, suggest top co-occuring symptoms to the user and ask to select the ones applicable \n",
        "found_symptoms=[]\n",
        "count=0\n",
        "for tup in dict_symp_tup:\n",
        "    count+=1\n",
        "    found_symptoms.append(tup[0])\n",
        "    if count%5==0 or count==len(dict_symp_tup):\n",
        "        print(\"\\nCommon co-occuring symptoms:\")\n",
        "        for idx,ele in enumerate(found_symptoms):\n",
        "            print(idx,\":\",ele)\n",
        "        select_list = input(\"Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\\n\").lower().split();\n",
        "        if select_list[0]=='no':\n",
        "            break\n",
        "        if select_list[0]=='-1':\n",
        "            found_symptoms = [] \n",
        "            continue\n",
        "        for idx in select_list:\n",
        "            final_symp.append(found_symptoms[int(idx)])\n",
        "        found_symptoms = []    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nI5taHc8pfY3"
      },
      "source": [
        "Final Symptom list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "colab_type": "code",
        "id": "YYPReN9D5Nd_",
        "outputId": "af695963-a0e3-4e76-c2a9-4a7c2ca75442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final list of Symptoms used for prediction are : \n"
          ]
        }
      ],
      "source": [
        "#Calculating TF-IDF and Cosine Similarity using matched symptoms\n",
        "k = 10\n",
        "\n",
        "print(\"Final list of Symptoms used for prediction are : \")\n",
        "for val in final_symp:\n",
        "    print(val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8_A-6Dl5qHlv"
      },
      "source": [
        "# **Showing the list of top k diseases to the user with their prediction probabilities.**\n",
        "\n",
        "# **For getting information about the suggested treatments, user can enter the corresponding index to know more details.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "colab_type": "code",
        "id": "tWUsVkF3t6jk",
        "outputId": "7baabb86-a3b9-4e7b-e7ce-fc4818ea9de3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Debug: TF_IDF Score Output (Top 10): []\n",
            "Debug: Cosine Similarity Output (Top 10): {np.int64(0): 0.0, np.int64(257): 0.0, np.int64(258): 0.0, np.int64(259): 0.0, np.int64(260): 0.0, np.int64(16): 0.0, np.int64(17): 0.0, np.int64(18): 0.0, np.int64(19): 0.0, np.int64(20): 0.0}\n",
            "\n",
            "Top 10 diseases predicted based on TF_IDF Matching:\n",
            "\n",
            "Invalid input! Please enter a valid index number or '-1' to exit.\n",
            "Exiting. Thank you!\n"
          ]
        }
      ],
      "source": [
        "# Debugging topk1 and topk2 for proper data output\n",
        "topk1 = tf_idf_score(k, final_symp)  # Example output: [(\"Disease1\", 0.89), (\"Disease2\", 0.78)]\n",
        "topk2 = cosine_similarity(k, final_symp)  # Example output: Similar structure to topk1\n",
        "\n",
        "# Verify that topk1 is sorted correctly and contains tuples\n",
        "print(f\"Debug: TF_IDF Score Output (Top {k}): {topk1}\")\n",
        "print(f\"Debug: Cosine Similarity Output (Top {k}): {topk2}\")\n",
        "\n",
        "# Show top k highly probable diseases to the user\n",
        "print(f\"\\nTop {k} diseases predicted based on TF_IDF Matching:\\n\")\n",
        "topk1_index_mapping = {}  # Mapping index to disease name\n",
        "\n",
        "# Check if topk1 is iterable and loop through it\n",
        "try:\n",
        "    for i, (key, score) in enumerate(topk1):\n",
        "        print(f\"{i}. Disease: {key} \\t Score: {round(score, 2)}\")\n",
        "        topk1_index_mapping[i] = key\n",
        "except Exception as e:\n",
        "    print(f\"Error iterating through topk1: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Get user input safely\n",
        "while True:\n",
        "    try:\n",
        "        select = input(\"\\nMore details about the disease? Enter index of disease or '-1' to discontinue:\\n\")\n",
        "        if select == '-1':\n",
        "            print(\"Exiting. Thank you!\")\n",
        "            break\n",
        "\n",
        "        # Validate the input index\n",
        "        index = int(select)\n",
        "        if index in topk1_index_mapping:\n",
        "            dis = topk1_index_mapping[index]\n",
        "            print(\"\\nFetching details for the selected disease...\\n\")\n",
        "            print(diseaseDetail(dis))\n",
        "        else:\n",
        "            print(f\"Invalid index: {index}. Please enter a valid index between 0 and {len(topk1) - 1}.\")\n",
        "    except ValueError:\n",
        "        print(\"Invalid input! Please enter a valid index number or '-1' to exit.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "colab_type": "code",
        "id": "AHiGmHhqdnMs",
        "outputId": "70f583d0-83e5-4424-8849-c97fbb189c41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 disease based on Cosine Similarity Matching :\n",
            " \n",
            "0. Disease : Abscess \t Score : 0.0\n",
            "1. Disease : Yellow Fever \t Score : 0.0\n",
            "2. Disease : Zika virus disease \t Score : 0.0\n",
            "3. Disease : lactose intolerance \t Score : 0.0\n",
            "4. Disease : papilloedema \t Score : 0.0\n",
            "5. Disease : Anxiety \t Score : 0.0\n",
            "6. Disease : Appendicitis \t Score : 0.0\n",
            "7. Disease : Arthritis \t Score : 0.0\n",
            "8. Disease : Asbestos-related diseases \t Score : 0.0\n",
            "9. Disease : Aseptic meningitis \t Score : 0.0\n"
          ]
        }
      ],
      "source": [
        "# display top k diseases predicted with cosine probablity\n",
        "print(f\"Top {k} disease based on Cosine Similarity Matching :\\n \")\n",
        "topk2_sorted = dict(sorted(topk2.items(), key=lambda kv: kv[1], reverse=True))\n",
        "j = 0\n",
        "topk2_index_mapping = {}\n",
        "for key in topk2_sorted:\n",
        "  print(f\"{j}. Disease : {diseases[key]} \\t Score : {round(topk2_sorted[key], 2)}\")\n",
        "  topk2_index_mapping[j] = diseases[key]\n",
        "  j += 1\n",
        "\n",
        "    \n",
        "select = input(\"\\nMore details about the disease? Enter index of disease or '-1' to discontinue and close the system:\\n\")\n",
        "if select!='-1':\n",
        "    dis=topk2_index_mapping[int(select)]\n",
        "    print()\n",
        "    print(diseaseDetail(dis))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ohuPaboMyKXa"
      },
      "source": [
        "# New Section\n",
        "**NEURAL_NETWORK AND GAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "colab_type": "code",
        "id": "p9pStYfV_FGM",
        "outputId": "b509778c-015e-4f20-8e65-be400cc81d20"
      },
      "outputs": [],
      "source": [
        "# \n",
        "# !pip install neural_structured_learning\n",
        "#importing all libraries\n",
        "# import neural_structured_learning as nsl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras import initializers\n",
        "from keras.optimizers import SGD\n",
        "#import neural_structured_learning as nsl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4TQqjkbmymrb"
      },
      "outputs": [],
      "source": [
        "#reading Dataset and making dataframe\n",
        "datat=pd.read_csv('./Dataset/dis_sym_dataset_comb.csv')\n",
        "df_new=pd.DataFrame(datat)\n",
        "df_new=df_new.sample(frac=1)\n",
        "#print(df_new)\n",
        "Y=df_new['label_dis']\n",
        "X=df_new.drop(columns='label_dis',axis=1)\n",
        "total_symptoms_len=len(X.columns)\n",
        "total_disease_len=len(set(Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AJGNzECTynHS"
      },
      "outputs": [],
      "source": [
        "#Label Encoding Class to numeric type \n",
        "#Converting class to categorical type for categorical cross entropy\n",
        "lb=LabelEncoder()\n",
        "Y=lb.fit_transform(Y)\n",
        "Ycat=to_categorical(Y)\n",
        "X=np.array(X)\n",
        "Y=np.array(Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iB0KNQoRynNL"
      },
      "outputs": [],
      "source": [
        "#importing tensorflow and keras frameworks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "#base Model Neurel Net\n",
        "def base_model():\n",
        "  inputs=keras.Input(shape=(total_symptoms_len,),dtype=tf.float32,name=IMAGE_INPUT_NAME)#defining input shape and dtype \n",
        "  x=inputs\n",
        "  x=keras.layers.Dense(1000,activation='relu',use_bias=True,kernel_initializer=initializers.he_normal(seed=None))(x)#Dense layer relu\n",
        "\n",
        "  x=keras.layers.Dense(1000,activation='relu',use_bias=True,kernel_initializer=initializers.he_normal(seed=None))(x)#Dense layer relu\n",
        "\n",
        "  outputs=keras.layers.Dense(total_disease_len,activation='softmax')(x)#output Dense layer with class size\n",
        "\n",
        "  model=keras.Model(inputs=inputs,outputs=outputs,name='NN_sequential_model')#creating model\n",
        "\n",
        "  #model.add(Dense(1500,activation='relu',kernel_initializer='he_uniform'))\n",
        "  # model.add(Dense(500,activation='relu',use_bias=True,kernel_initializer=initializers.he_normal(seed=None)))\n",
        "  # model.add(Dense(183,activation='softmax'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7xYNar3nynKf"
      },
      "outputs": [],
      "source": [
        "def convert_to_dictionaries(image, label):\n",
        "  return {IMAGE_INPUT_NAME: image, LABEL_INPUT_NAME: label}\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iHSec4ggynSr"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'estimator' from 'tensorflow' (d:\\ml\\venv\\Lib\\site-packages\\tensorflow\\__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mdisable_v2_behavior()  \u001b[38;5;66;03m# Disable TensorFlow 2.x behavior\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mneural_structured_learning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnsl\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
            "File \u001b[1;32md:\\ml\\venv\\Lib\\site-packages\\neural_structured_learning\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Subpackages of Neural Structured Learning.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_structured_learning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configs\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_structured_learning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_structured_learning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_structured_learning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n",
            "File \u001b[1;32md:\\ml\\venv\\Lib\\site-packages\\neural_structured_learning\\estimator\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Estimator APIs for Neural Structured Learning.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThe current NSL Estimator APIs may not be compatible with certain TensorFlow\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mversions (such as 2.0 or above).\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_structured_learning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madversarial_regularization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_adversarial_regularization\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_structured_learning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_regularization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_graph_regularization\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_adversarial_regularization\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_graph_regularization\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[1;32md:\\ml\\venv\\Lib\\site-packages\\neural_structured_learning\\estimator\\adversarial_regularization.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mneural_structured_learning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnsl_lib\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator \u001b[38;5;28;01mas\u001b[39;00m tf_estimator\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_adversarial_regularization\u001b[39m(estimator,\n\u001b[0;32m     31\u001b[0m                                    optimizer_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m                                    adv_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Adds adversarial regularization to a `tf.estimator.Estimator`.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m  The returned estimator will include the adversarial loss as a regularization\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    incorporated into its loss.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'estimator' from 'tensorflow' (d:\\ml\\venv\\Lib\\site-packages\\tensorflow\\__init__.py)"
          ]
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()  # Disable TensorFlow 2.x behavior\n",
        "\n",
        "import neural_structured_learning as nsl\n",
        "\n",
        "from tf.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# Define the base model (using Keras for TensorFlow 2.x)\n",
        "def base_model(input_shape=(224, 224, 3)):  # Example input shape for images\n",
        "    model = models.Sequential([\n",
        "        layers.InputLayer(input_shape=input_shape, name='image'),  # Image input\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax', name='label')  # Change output classes as needed\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Adversarial regularization configuration\n",
        "adv_config = nsl.configs.make_adv_reg_config(\n",
        "    multiplier=0.2,         # Strength of adversarial regularization\n",
        "    adv_step_size=0.0001    # Step size for adversarial perturbations\n",
        ")\n",
        "\n",
        "# Initialize the base model\n",
        "base_adv_model = base_model()\n",
        "\n",
        "# Apply adversarial regularization to the base model\n",
        "adv_model = nsl.keras.AdversarialRegularization(\n",
        "    base_adv_model,                        # Base model\n",
        "    label_keys=['label'],                  # The label key for adversarial regularization\n",
        "    adv_config=adv_config                  # Adversarial configuration\n",
        ")\n",
        "\n",
        "# Function to convert data to the required dictionary format\n",
        "def convert_to_dictionaries(X, Y):\n",
        "    \"\"\"Convert data to a format that NSL expects.\"\"\"\n",
        "    return [{'image': X[i], 'label': Y[i]} for i in range(len(X))]\n",
        "\n",
        "# Example data (replace with actual data)\n",
        "X = np.random.rand(100, 224, 224, 3)  # 100 images of shape 224x224 with 3 channels (RGB)\n",
        "Y = np.random.randint(0, 10, size=(100,))  # 100 labels (for 10 classes)\n",
        "\n",
        "# Convert dataset to dictionaries for adversarial training\n",
        "train_set_for_adv_model = convert_to_dictionaries(X, Y)\n",
        "\n",
        "# Train the adversarial model using the dataset\n",
        "adv_model.fit(train_set_for_adv_model, epochs=10, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "AsVGBV7YynVG",
        "outputId": "3094a75e-a221-47bd-f933-5f9d82052617"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'IMAGE_INPUT_NAME' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m base_mod\u001b[38;5;241m=\u001b[39m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m base_mod\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m      3\u001b[0m es\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;66;03m#early stopping\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[20], line 11\u001b[0m, in \u001b[0;36mbase_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbase_model\u001b[39m():\n\u001b[1;32m---> 11\u001b[0m   inputs\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(total_symptoms_len,),dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32,name\u001b[38;5;241m=\u001b[39m\u001b[43mIMAGE_INPUT_NAME\u001b[49m)\u001b[38;5;66;03m#defining input shape and dtype \u001b[39;00m\n\u001b[0;32m     12\u001b[0m   x\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m     13\u001b[0m   x\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1000\u001b[39m,activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,kernel_initializer\u001b[38;5;241m=\u001b[39minitializers\u001b[38;5;241m.\u001b[39mhe_normal(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))(x)\u001b[38;5;66;03m#Dense layer relu\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'IMAGE_INPUT_NAME' is not defined"
          ]
        }
      ],
      "source": [
        "base_mod=base_model()\n",
        "base_mod.summary()\n",
        "es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=10)#early stopping\n",
        "mc = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)#saving best model\n",
        "print(\"Normal Feed Forward Neural Network\")\n",
        "base_mod.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history=base_mod.fit(X,Ycat,validation_split=0.2,epochs=20,verbose=1,callbacks=[es,mc])#training Neural Network\n",
        "base_mod.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "OPNvVLv1ymql",
        "outputId": "4a840e11-68a2-4e42-fbf1-56979a09201c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "applied adversarial regularization on base neural network\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int64\n",
            "WARNING:tensorflow:Cannot perturb feature image\n",
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int64\n",
            "WARNING:tensorflow:Cannot perturb feature image\n",
            "221/221 [==============================] - ETA: 0s - loss: 1.8495 - categorical_crossentropy: 1.5413 - categorical_accuracy: 0.7349 - adversarial_loss: 1.5413WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int64\n",
            "WARNING:tensorflow:Cannot perturb feature image\n",
            "\n",
            "Epoch 00001: val_categorical_accuracy improved from -inf to 0.85795, saving model to best_model.h5\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 1.8495 - categorical_crossentropy: 1.5413 - categorical_accuracy: 0.7349 - adversarial_loss: 1.5413 - val_loss: 0.8672 - val_categorical_crossentropy: 0.7227 - val_categorical_accuracy: 0.8580 - val_adversarial_loss: 0.7227\n",
            "Epoch 2/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.6026 - categorical_crossentropy: 0.5022 - categorical_accuracy: 0.8861 - adversarial_loss: 0.5022\n",
            "Epoch 00002: val_categorical_accuracy improved from 0.85795 to 0.87832, saving model to best_model.h5\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.6026 - categorical_crossentropy: 0.5022 - categorical_accuracy: 0.8861 - adversarial_loss: 0.5022 - val_loss: 0.6341 - val_categorical_crossentropy: 0.5284 - val_categorical_accuracy: 0.8783 - val_adversarial_loss: 0.5284\n",
            "Epoch 3/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.4426 - categorical_crossentropy: 0.3688 - categorical_accuracy: 0.9005 - adversarial_loss: 0.3688\n",
            "Epoch 00003: val_categorical_accuracy improved from 0.87832 to 0.88398, saving model to best_model.h5\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.4426 - categorical_crossentropy: 0.3688 - categorical_accuracy: 0.9005 - adversarial_loss: 0.3688 - val_loss: 0.6199 - val_categorical_crossentropy: 0.5166 - val_categorical_accuracy: 0.8840 - val_adversarial_loss: 0.5166\n",
            "Epoch 4/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3922 - categorical_crossentropy: 0.3269 - categorical_accuracy: 0.9055 - adversarial_loss: 0.3269\n",
            "Epoch 00004: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3922 - categorical_crossentropy: 0.3269 - categorical_accuracy: 0.9055 - adversarial_loss: 0.3269 - val_loss: 0.6243 - val_categorical_crossentropy: 0.5202 - val_categorical_accuracy: 0.8766 - val_adversarial_loss: 0.5202\n",
            "Epoch 5/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3695 - categorical_crossentropy: 0.3079 - categorical_accuracy: 0.9045 - adversarial_loss: 0.3079\n",
            "Epoch 00005: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 30ms/step - loss: 0.3695 - categorical_crossentropy: 0.3079 - categorical_accuracy: 0.9045 - adversarial_loss: 0.3079 - val_loss: 0.6183 - val_categorical_crossentropy: 0.5152 - val_categorical_accuracy: 0.8823 - val_adversarial_loss: 0.5152\n",
            "Epoch 6/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3468 - categorical_crossentropy: 0.2890 - categorical_accuracy: 0.9090 - adversarial_loss: 0.2890\n",
            "Epoch 00006: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3468 - categorical_crossentropy: 0.2890 - categorical_accuracy: 0.9090 - adversarial_loss: 0.2890 - val_loss: 0.5916 - val_categorical_crossentropy: 0.4930 - val_categorical_accuracy: 0.8812 - val_adversarial_loss: 0.4930\n",
            "Epoch 7/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3380 - categorical_crossentropy: 0.2816 - categorical_accuracy: 0.9085 - adversarial_loss: 0.2816\n",
            "Epoch 00007: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 30ms/step - loss: 0.3380 - categorical_crossentropy: 0.2816 - categorical_accuracy: 0.9085 - adversarial_loss: 0.2816 - val_loss: 0.5980 - val_categorical_crossentropy: 0.4983 - val_categorical_accuracy: 0.8823 - val_adversarial_loss: 0.4983\n",
            "Epoch 8/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3307 - categorical_crossentropy: 0.2756 - categorical_accuracy: 0.9100 - adversarial_loss: 0.2756\n",
            "Epoch 00008: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3307 - categorical_crossentropy: 0.2756 - categorical_accuracy: 0.9100 - adversarial_loss: 0.2756 - val_loss: 0.5968 - val_categorical_crossentropy: 0.4973 - val_categorical_accuracy: 0.8812 - val_adversarial_loss: 0.4973\n",
            "Epoch 9/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3271 - categorical_crossentropy: 0.2725 - categorical_accuracy: 0.9063 - adversarial_loss: 0.2725\n",
            "Epoch 00009: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 30ms/step - loss: 0.3271 - categorical_crossentropy: 0.2725 - categorical_accuracy: 0.9063 - adversarial_loss: 0.2725 - val_loss: 0.6140 - val_categorical_crossentropy: 0.5117 - val_categorical_accuracy: 0.8800 - val_adversarial_loss: 0.5117\n",
            "Epoch 10/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3194 - categorical_crossentropy: 0.2662 - categorical_accuracy: 0.9089 - adversarial_loss: 0.2662\n",
            "Epoch 00010: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3194 - categorical_crossentropy: 0.2662 - categorical_accuracy: 0.9089 - adversarial_loss: 0.2662 - val_loss: 0.6442 - val_categorical_crossentropy: 0.5368 - val_categorical_accuracy: 0.8806 - val_adversarial_loss: 0.5368\n",
            "Epoch 11/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3189 - categorical_crossentropy: 0.2658 - categorical_accuracy: 0.9063 - adversarial_loss: 0.2658\n",
            "Epoch 00011: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3189 - categorical_crossentropy: 0.2658 - categorical_accuracy: 0.9063 - adversarial_loss: 0.2658 - val_loss: 0.6235 - val_categorical_crossentropy: 0.5195 - val_categorical_accuracy: 0.8806 - val_adversarial_loss: 0.5195\n",
            "Epoch 12/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3141 - categorical_crossentropy: 0.2618 - categorical_accuracy: 0.9058 - adversarial_loss: 0.2618\n",
            "Epoch 00012: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 30ms/step - loss: 0.3141 - categorical_crossentropy: 0.2618 - categorical_accuracy: 0.9058 - adversarial_loss: 0.2618 - val_loss: 0.6167 - val_categorical_crossentropy: 0.5139 - val_categorical_accuracy: 0.8834 - val_adversarial_loss: 0.5139\n",
            "Epoch 13/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3111 - categorical_crossentropy: 0.2593 - categorical_accuracy: 0.9070 - adversarial_loss: 0.2593\n",
            "Epoch 00013: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3111 - categorical_crossentropy: 0.2593 - categorical_accuracy: 0.9070 - adversarial_loss: 0.2593 - val_loss: 0.6124 - val_categorical_crossentropy: 0.5103 - val_categorical_accuracy: 0.8795 - val_adversarial_loss: 0.5103\n",
            "Epoch 14/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3032 - categorical_crossentropy: 0.2526 - categorical_accuracy: 0.9110 - adversarial_loss: 0.2526\n",
            "Epoch 00014: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3032 - categorical_crossentropy: 0.2526 - categorical_accuracy: 0.9110 - adversarial_loss: 0.2526 - val_loss: 0.6401 - val_categorical_crossentropy: 0.5334 - val_categorical_accuracy: 0.8789 - val_adversarial_loss: 0.5334\n",
            "Epoch 15/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.2996 - categorical_crossentropy: 0.2496 - categorical_accuracy: 0.9082 - adversarial_loss: 0.2496\n",
            "Epoch 00015: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.2996 - categorical_crossentropy: 0.2496 - categorical_accuracy: 0.9082 - adversarial_loss: 0.2496 - val_loss: 0.6326 - val_categorical_crossentropy: 0.5271 - val_categorical_accuracy: 0.8812 - val_adversarial_loss: 0.5271\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff7259134a8>"
            ]
          },
          "execution_count": 221,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adv_model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                   metrics=['acc'])\n",
        "es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=10)\n",
        "mc = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_categorical_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "print(\"applied adversarial regularization on base neural network\")\n",
        "#adv_model.compile(optimizer='adam', loss='categorical_cross_entropy', metrics=['accuracy'])\n",
        "adv_model.fit(train_set_for_adv_model,validation_split=0.2 ,epochs=15,callbacks=[es,mc])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TF_IDF_NN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
